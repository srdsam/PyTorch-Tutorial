{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training dataset\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x124B66810>, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Tells jupyter to plot graphs in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMbklEQVR4nO3db4gc9R3H8c8n1iJE0WjoGTU1bfFJKTaWIIUeJcU0RBGSPgnNgxKp9PqgSgsVIlaoUgqhVouIClc0f4pVhGgTSmlrQ9SWoHhKqlGTakOCOeJdRaTmUar37YOdyBlvZ8+dmZ1Nvu8XHLs7392ZL0M+mX+783NECMCZb0HbDQAYDMIOJEHYgSQIO5AEYQeS+MwgF2abU/9AwyLCc02vtGW3vcb2Qdtv2r61yrwANMv9Xme3fZakf0n6tqSjkl6QtCEiXiv5DFt2oGFNbNmvlvRmRByKiBOSHpO0tsL8ADSoStgvlfTWrNdHi2kfY3vM9oTtiQrLAlBR4yfoImJc0rjEbjzQpipb9klJS2e9vqyYBmAIVQn7C5KusP0F25+V9F1Ju+ppC0Dd+t6Nj4gPbN8k6S+SzpL0cES8WltnAGrV96W3vhbGMTvQuEa+VAPg9EHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEn0P2Qw07fbbby+t33nnnaX1BQu6b8tWrlxZ+tlnnnmmtH46qhR224clvS/pQ0kfRMSKOpoCUL86tuzfioh3apgPgAZxzA4kUTXsIemvtl+0PTbXG2yP2Z6wPVFxWQAqqLobPxoRk7Y/J+kp2wci4tnZb4iIcUnjkmQ7Ki4PQJ8qbdkjYrJ4nJb0pKSr62gKQP36DrvthbbPO/lc0mpJ++tqDEC9quzGj0h60vbJ+fw+Iv5cS1dI4YYbbiitb9q0qbQ+MzPT97Ij8h1R9h32iDgk6as19gKgQVx6A5Ig7EAShB1IgrADSRB2IAl+4orWXH755aX1c845Z0Cd5MCWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Do7GrVq1aqutZtvvrnSvA8cOFBav/7667vWpqamKi37dMSWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Do7KhkdHS2tb9mypWvt/PPPr7Tsu+66q7R+5MiRSvM/07BlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkuM6OSjZu3Fhav+SSS/qe99NPP11a3759e9/zzqjnlt32w7anbe+fNe1C20/ZfqN4XNRsmwCqms9u/FZJa06Zdquk3RFxhaTdxWsAQ6xn2CPiWUnvnjJ5raRtxfNtktbV2xaAuvV7zD4SEceK529LGun2Rttjksb6XA6AmlQ+QRcRYTtK6uOSxiWp7H0AmtXvpbcp20skqXicrq8lAE3oN+y7JJ285rJR0s562gHQFEeU71nbflTSSkmLJU1J+rmkP0h6XNLnJR2RtD4iTj2JN9e82I0/zSxevLi03uv+6zMzM11r7733Xuln169fX1rfs2dPaT2riPBc03ses0fEhi6layp1BGCg+LoskARhB5Ig7EAShB1IgrADSfAT1+SWLVtWWt+xY0djy77vvvtK61xaqxdbdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1Iguvsya1Zc+q9RD/uyiuvrDT/3bt3d63de++9leaNT4ctO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0fNW0rUujFtJD9y6detK61u3bi2tL1y4sLS+d+/e0nrZ7aB73YYa/el2K2m27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBL9nPwOU3fu9yfu+S9KhQ4dK61xLHx49t+y2H7Y9bXv/rGl32J60va/4u67ZNgFUNZ/d+K2S5rqdyW8iYnnx96d62wJQt55hj4hnJb07gF4ANKjKCbqbbL9c7OYv6vYm22O2J2xPVFgWgIr6DfuDkr4kabmkY5Lu7vbGiBiPiBURsaLPZQGoQV9hj4ipiPgwImYk/VbS1fW2BaBufYXd9pJZL78jaX+39wIYDj2vs9t+VNJKSYttH5X0c0krbS+XFJIOS/phcy2il02bNnWtzczMNLrszZs3Nzp/1Kdn2CNiwxyTH2qgFwAN4uuyQBKEHUiCsANJEHYgCcIOJMFPXE8Dy5cvL62vXr26sWXv3LmztH7w4MHGlo16sWUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYsvk0MD09XVpftKjrXcF6eu6550rr1157bWn9+PHjfS8bzWDIZiA5wg4kQdiBJAg7kARhB5Ig7EAShB1Igt+znwYuuuii0nqV20U/8MADpXWuo5852LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJcZx8CW7ZsKa0vWNDc/8l79+5tbN4YLj3/FdleanuP7ddsv2r7x8X0C20/ZfuN4rH/OygAaNx8NhkfSPppRHxZ0tcl/cj2lyXdKml3RFwhaXfxGsCQ6hn2iDgWES8Vz9+X9LqkSyWtlbSteNs2Sesa6hFADT7VMbvtZZKukvS8pJGIOFaU3pY00uUzY5LGKvQIoAbzPvNj+1xJOyT9JCL+O7sWnbtWznkzyYgYj4gVEbGiUqcAKplX2G2frU7QH4mIJ4rJU7aXFPUlkspvgQqgVT13421b0kOSXo+Ie2aVdknaKGlz8Vg+tm9ivYZcXrVqVWm9109YT5w40bV2//33l352amqqtI4zx3yO2b8h6XuSXrG9r5h2mzohf9z2jZKOSFrfSIcAatEz7BHxD0lz3nRe0jX1tgOgKXxdFkiCsANJEHYgCcIOJEHYgST4iesAXHDBBaX1iy++uNL8Jycnu9ZuueWWSvPGmYMtO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB79kH4MCBA6X1XsMmj46O1tkOkmLLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJOCLK32AvlbRd0oikkDQeEffavkPSDyT9p3jrbRHxpx7zKl8YgMoiYs5Rl+cT9iWSlkTES7bPk/SipHXqjMd+PCJ+Pd8mCDvQvG5hn8/47MckHSuev2/7dUmX1tsegKZ9qmN228skXSXp+WLSTbZftv2w7UVdPjNme8L2RLVWAVTRczf+ozfa50p6RtIvI+IJ2yOS3lHnOP4X6uzqf7/HPNiNBxrW9zG7JNk+W9IfJf0lIu6Zo75M0h8j4is95kPYgYZ1C3vP3XjblvSQpNdnB704cXfSdyTtr9okgObM52z8qKS/S3pF0kwx+TZJGyQtV2c3/rCkHxYn88rmxZYdaFil3fi6EHageX3vxgM4MxB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSGPSQze9IOjLr9eJi2jAa1t6GtS+J3vpVZ2+XdysM9Pfsn1i4PRERK1proMSw9jasfUn01q9B9cZuPJAEYQeSaDvs4y0vv8yw9jasfUn01q+B9NbqMTuAwWl7yw5gQAg7kEQrYbe9xvZB22/avrWNHrqxfdj2K7b3tT0+XTGG3rTt/bOmXWj7KdtvFI9zjrHXUm932J4s1t0+29e11NtS23tsv2b7Vds/Lqa3uu5K+hrIehv4MbvtsyT9S9K3JR2V9IKkDRHx2kAb6cL2YUkrIqL1L2DY/qak45K2nxxay/avJL0bEZuL/ygXRcSmIentDn3KYbwb6q3bMOM3qMV1V+fw5/1oY8t+taQ3I+JQRJyQ9JiktS30MfQi4llJ754yea2kbcXzber8Yxm4Lr0NhYg4FhEvFc/fl3RymPFW111JXwPRRtgvlfTWrNdHNVzjvYekv9p+0fZY283MYWTWMFtvSxpps5k59BzGe5BOGWZ8aNZdP8OfV8UJuk8ajYivSbpW0o+K3dWhFJ1jsGG6dvqgpC+pMwbgMUl3t9lMMcz4Dkk/iYj/zq61ue7m6Gsg662NsE9KWjrr9WXFtKEQEZPF47SkJ9U57BgmUydH0C0ep1vu5yMRMRURH0bEjKTfqsV1VwwzvkPSIxHxRDG59XU3V1+DWm9thP0FSVfY/oLtz0r6rqRdLfTxCbYXFidOZHuhpNUavqGod0naWDzfKGlni718zLAM491tmHG1vO5aH/48Igb+J+k6dc7I/1vSz9rooUtfX5T0z+Lv1bZ7k/SoOrt1/1Pn3MaNki6StFvSG5L+JunCIertd+oM7f2yOsFa0lJvo+rsor8saV/xd13b666kr4GsN74uCyTBCTogCcIOJEHYgSQIO5AEYQeSIOxAEoQdSOL/n+rnSfOvm60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[3]\n",
    "plt.imshow(image, cmap='gray') # color map == grayscale image\n",
    "print(\"Label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert these images into tensors\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='data/',\n",
    "               train=True,\n",
    "               transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 1\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[3]\n",
    "print(img_tensor.shape, label)\n",
    "# 1 dimensional image, 28 by 28. Would be 3 dimensions for RBG image\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12830cb50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0,10:20,10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3 'Groups'\n",
    "Training set - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n",
    "Validation set - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n",
    "Test set - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n",
    "\n",
    "Additionally shuffle data so that the program recognises all numbers, rather than just 0-7.\n",
    "Do not shuffle time data\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_indices(n, val_pct):\n",
    "    # determine size of validation set\n",
    "    n_val = int(val_pct*n)\n",
    "    # Create random permutation of 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    # Pick first n_val indices for validation set\n",
    "    return idxs[n_val:], idxs[:n_val]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, val_indices = split_indices(len(dataset), val_pct=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_indices), len(val_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30826 38754 21550 16814 30597 24014  2442 25154 57213 33540 16295 54063\n",
      " 20632 50746 19131   335 15204 26352  2857 26077]\n"
     ]
    }
   ],
   "source": [
    "print(val_indices[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small batch size to limit number of calculations required\n",
    "batch_size=100\n",
    "\n",
    "# Training sampler and data loader \n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_loader = DataLoader(dataset,\n",
    "                         batch_size,\n",
    "                         sampler=train_sampler)\n",
    "\n",
    "# Validation sampler and data loader\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "val_loader = DataLoader(dataset,\n",
    "                       batch_size,\n",
    "                       sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use nn.Linear, however we must convert each image into a vector so it can be used\n",
    "# images are 28x28 so vector is 784. \n",
    "# Output will be a vector of size 10, each item will be probability of the model being that image\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Log regression model\n",
    "model = nn.Linear(input_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0345, -0.0097, -0.0099,  ..., -0.0207, -0.0301,  0.0327],\n",
       "        [ 0.0153,  0.0324, -0.0313,  ..., -0.0109, -0.0235,  0.0235],\n",
       "        [-0.0206, -0.0182,  0.0045,  ...,  0.0176,  0.0352, -0.0110],\n",
       "        ...,\n",
       "        [ 0.0163,  0.0181,  0.0329,  ...,  0.0060,  0.0355,  0.0182],\n",
       "        [-0.0323,  0.0286,  0.0011,  ...,  0.0175,  0.0038,  0.0069],\n",
       "        [-0.0171,  0.0226,  0.0235,  ...,  0.0350, -0.0276,  0.0256]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0072,  0.0129, -0.0205,  0.0219, -0.0058, -0.0346,  0.0258, -0.0352,\n",
       "         0.0178,  0.0138], requires_grad=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 8, 3, 6, 8, 9, 5, 5, 5, 4, 1, 3, 0, 3, 4, 4, 7, 1, 9, 5, 7, 1, 4, 5,\n",
      "        9, 2, 8, 7, 9, 0, 4, 3, 9, 2, 7, 2, 9, 5, 5, 1, 0, 2, 7, 2, 6, 8, 2, 4,\n",
      "        4, 8, 1, 1, 8, 6, 1, 1, 1, 7, 1, 9, 5, 4, 1, 6, 4, 8, 5, 9, 7, 7, 2, 9,\n",
      "        5, 7, 0, 0, 1, 1, 9, 0, 7, 0, 3, 8, 5, 3, 5, 6, 3, 0, 4, 5, 9, 2, 8, 6,\n",
      "        1, 2, 9, 5])\n",
      "torch.Size([100, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [2800 x 28], m2: [784 x 10] at /tmp/pip-req-build-9oilk29k/aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-338f1c812363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# the label is the correct number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [2800 x 28], m2: [784 x 10] at /tmp/pip-req-build-9oilk29k/aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    # the label is the correct number\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break\n",
    "    \n",
    "# our images are in the wrong shape (look at size). Must flatten 1, 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend nn.Module class from pytorch. Custom model\n",
    "\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # super constructor for inheriting class\n",
    "        super().__init__()\n",
    "        # Model inside module, nested structure\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    # Must define forward method in any pytorch method\n",
    "    def forward(self, xb):\n",
    "        # remember -1 means choose what dimension should be\n",
    "        # Allows model to stay generalized, i.e. batchsize can change\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        # Pass reshaped matrix into linear model\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MnistModel()\n",
    "# Weight and bias inside model.linear now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0326, -0.0038,  0.0332,  ..., -0.0114,  0.0008, -0.0156],\n",
       "         [-0.0214,  0.0209,  0.0090,  ..., -0.0185,  0.0250,  0.0246],\n",
       "         [ 0.0242, -0.0151,  0.0147,  ..., -0.0128,  0.0336,  0.0153],\n",
       "         ...,\n",
       "         [ 0.0234, -0.0328,  0.0245,  ...,  0.0243, -0.0167,  0.0180],\n",
       "         [-0.0158,  0.0271, -0.0006,  ..., -0.0176,  0.0312, -0.0175],\n",
       "         [ 0.0156,  0.0042,  0.0074,  ..., -0.0172, -0.0204, -0.0060]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0031, -0.0085, -0.0328,  0.0210,  0.0287,  0.0337,  0.0210,  0.0242,\n",
       "         -0.0098,  0.0103], requires_grad=True)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10])\n",
      "tensor([[ 0.3415, -0.0076, -0.5497,  0.2518,  0.3781,  0.0897,  0.1789, -0.1493,\n",
      "          0.0113, -0.1751],\n",
      "        [ 0.1232, -0.0383, -0.4656, -0.0722,  0.2571, -0.0189,  0.1212, -0.1224,\n",
      "          0.0744, -0.2077]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "    \n",
    "print(outputs.shape) \n",
    "print(outputs[:2].data) # All these should be 0-1 and add up to 1 (not probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use softmax to convert Logits scores into probabilities if you want - easy implementation\n",
    "# There is also a pyTorch method\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:  tensor([[0.1312, 0.0925, 0.0538, 0.1199, 0.1361, 0.1020, 0.1115, 0.0803, 0.0943,\n",
      "         0.0783],\n",
      "        [0.1151, 0.0979, 0.0639, 0.0947, 0.1316, 0.0998, 0.1148, 0.0900, 0.1096,\n",
      "         0.0827]])\n",
      "Sum of probabilites tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "print(\"Probabilities: \", probs[:2].data)\n",
    "# Sometimes there is floating point error with the sum. \n",
    "print(\"Sum of probabilites\", torch.sum(probs[0].data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 3, 0, 4, 3, 3, 4, 5, 0, 1, 0, 4, 6, 3, 0, 0, 9, 7, 0, 4, 4, 4, 3,\n",
      "        5, 4, 4, 0, 3, 3, 4, 4, 3, 8, 0, 0, 4, 4, 5, 4, 4, 0, 3, 3, 4, 4, 4, 6,\n",
      "        0, 4, 4, 3, 3, 5, 4, 3, 3, 1, 4, 0, 4, 6, 6, 4, 4, 7, 4, 3, 3, 4, 0, 3,\n",
      "        4, 4, 3, 4, 0, 4, 4, 7, 0, 4, 9, 4, 0, 6, 4, 3, 4, 0, 4, 0, 3, 4, 3, 0,\n",
      "        4, 3, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "# Select the prediction based on the max probability element\n",
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "# print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 3, 5, 9, 5, 1, 2, 5, 6, 8, 7, 7, 5, 7, 8, 5, 4, 1, 7, 3, 8, 4, 6, 6,\n",
       "        3, 0, 6, 1, 8, 3, 0, 7, 2, 3, 1, 1, 3, 4, 1, 0, 8, 9, 6, 8, 3, 8, 2, 4,\n",
       "        1, 4, 5, 2, 2, 2, 9, 9, 3, 7, 8, 7, 3, 1, 5, 8, 8, 9, 0, 3, 1, 5, 2, 9,\n",
       "        5, 0, 9, 5, 8, 0, 0, 4, 6, 3, 4, 5, 5, 7, 7, 1, 0, 0, 3, 7, 3, 0, 6, 8,\n",
       "        8, 6, 8, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels\n",
    "# Model still poor, as no fine tuned weights or biases yet. Need to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrix and loss function\n",
    "def accuracy(preds, labels):\n",
    "    # This method uses the == method to return which predictions match up, \n",
    "    # and then divide it by length of the item to calculate % accuracy\n",
    "    return torch.sum(preds==labels).item()/len(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntorch.max and == are non-differentiable so cannot be used for predicting accuracy\\nAdditionally, this doesn't analyze what the model 'thinks' of the correct label\\nprobability. Feedback from loss function is not effective. Accuracy is good for \\nfinal result evaluation, but not a good loss function.\\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.max and == are non-differentiable so cannot be used for predicting accuracy\n",
    "Additionally, this doesn't analyze what the model 'thinks' of the correct label\n",
    "probability. Feedback from loss function is not effective. Accuracy is good for \n",
    "final result evaluation, but not a good loss function.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss function is used. \n",
    "\"\"\"\n",
    "When probability is low the (-) logarithm is higher, and vice versa.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3052, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Loss for current batch of data\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimiser \n",
    "learning_rate = .001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None):\n",
    "    # Calc loss\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        # compute gradients, update params, reset grads\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "        metric_result = metric(preds, yb)\n",
    "    \n",
    "    return loss.item(), len(xb), metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fn, valid_dl, metric=None):\n",
    "    with torch.no_grad():\n",
    "        results = [loss_batch(model, loss_fn, xb, yb, metric=metric)\n",
    "                 for xb,yb in valid_dl]\n",
    "        losses, nums, metrics = zip(*results)\n",
    "        total = np.sum(nums)\n",
    "        avg_loss = np.sum(np.multiply(losses, nums)/total)\n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "            avg_metric = np.sum(np.multiply(metrics,nums)) / total\n",
    "    return avg_loss, total, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds == labels).item() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.322481228907903 0.1115\n"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluate(model, loss_fn, val_loader, metric=accuracy)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dl, valid_dl, metric=None):\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        for xb, yb in train_dl:\n",
    "            loss,_,_ = loss_batch(model, loss_fn, xb, yb, opt)\n",
    "        # Evaluation\n",
    "        result = evaluate(model, loss_fn, valid_dl, metric)\n",
    "        val_loss, total, val_metric = result\n",
    "        \n",
    "        #Print progress\n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                 .format(epoch+1, epochs, val_loss))\n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "                 .format(epoch+1, epochs, val_loss, metric.__name__, val_metric))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MnistModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.8942, accuracy: 0.6466\n",
      "Epoch [2/5], Loss: 1.5920, accuracy: 0.7375\n",
      "Epoch [3/5], Loss: 1.3795, accuracy: 0.7705\n",
      "Epoch [4/5], Loss: 1.2270, accuracy: 0.7881\n",
      "Epoch [5/5], Loss: 1.1139, accuracy: 0.8008\n"
     ]
    }
   ],
   "source": [
    "fit(5, model, F.cross_entropy, optimizer, train_loader, val_loader, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIt is possible the learning rate isnt is too high, or more epochs are neccessary,\\nhowever it is most likely that the model isn't powerful enough. \\n\\nThere is probably not a linear relationship between pixel intensity and the\\nlabelled number. \\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy quickly jumps to 85% roughly but does not exceed 90%\n",
    "\"\"\"\n",
    "It is possible the learning rate isnt is too high, or more epochs are neccessary,\n",
    "however it is most likely that the model isn't powerful enough. \n",
    "\n",
    "There is probably not a linear relationship between pixel intensity and the\n",
    "labelled number. \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with individual image\n",
    "\n",
    "test_dataset = MNIST(root='data/',\n",
    "                    train=False,\n",
    "                    transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12cf2d590>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.unsqueeze(0).shape # prints unsqueezed shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0) # becomes a 'batch' of one image\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  3 , Predicted:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[90]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label: ', label, ', Predicted: ', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model cannot see a spatial relationship between pixels. Therefore is poor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model can be saved as\n",
    "\n",
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0055, -0.0139, -0.0163,  ...,  0.0026, -0.0009, -0.0057],\n",
       "                      [ 0.0135,  0.0339, -0.0308,  ...,  0.0104, -0.0175,  0.0227],\n",
       "                      [-0.0216, -0.0285,  0.0213,  ...,  0.0336,  0.0037, -0.0292],\n",
       "                      ...,\n",
       "                      [-0.0250, -0.0141,  0.0143,  ..., -0.0012,  0.0351, -0.0197],\n",
       "                      [ 0.0142, -0.0028,  0.0174,  ...,  0.0037, -0.0256, -0.0284],\n",
       "                      [-0.0184, -0.0129,  0.0293,  ..., -0.0061,  0.0033, -0.0147]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0279,  0.0573, -0.0257, -0.0231,  0.0087,  0.0371, -0.0351,  0.0426,\n",
       "                      -0.0156, -0.0177]))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0055, -0.0139, -0.0163,  ...,  0.0026, -0.0009, -0.0057],\n",
       "                      [ 0.0135,  0.0339, -0.0308,  ...,  0.0104, -0.0175,  0.0227],\n",
       "                      [-0.0216, -0.0285,  0.0213,  ...,  0.0336,  0.0037, -0.0292],\n",
       "                      ...,\n",
       "                      [-0.0250, -0.0141,  0.0143,  ..., -0.0012,  0.0351, -0.0197],\n",
       "                      [ 0.0142, -0.0028,  0.0174,  ...,  0.0037, -0.0256, -0.0284],\n",
       "                      [-0.0184, -0.0129,  0.0293,  ..., -0.0061,  0.0033, -0.0147]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.0279,  0.0573, -0.0257, -0.0231,  0.0087,  0.0371, -0.0351,  0.0426,\n",
       "                      -0.0156, -0.0177]))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MnistModel()\n",
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
